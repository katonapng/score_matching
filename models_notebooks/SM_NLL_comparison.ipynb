{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.insert(0, parent_dir)\n",
    "\n",
    "from dataloader import FastTensorDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1fddc2374f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_poisson_points(kappa, scale, region):\n",
    "    \"\"\"\n",
    "    Generate a Poisson Point Process in a 2D region based on intensity function.\n",
    "    \n",
    "    Parameters:\n",
    "    - kappa (torch.Tensor): The intensity parameter (scalar or vector).\n",
    "    - scale (torch.Tensor): The scale parameter (scalar or vector).\n",
    "    - region (tuple): The spatial domain as ((xmin, xmax), (ymin, ymax)).\n",
    "    \n",
    "    Returns:\n",
    "    - points (numpy.ndarray): The simulated points of the PPP.\n",
    "    \"\"\"\n",
    "    (xmin, xmax), (ymin, ymax) = region\n",
    "\n",
    "    area = (xmax - xmin) * (ymax - ymin)\n",
    "    max_intensity = kappa * area  # Maximum value of intensity\n",
    "    num_samples = np.random.poisson(lam=max_intensity)\n",
    "\n",
    "    x_candidates = np.random.uniform(xmin, xmax, size=num_samples)\n",
    "    y_candidates = np.random.uniform(ymin, ymax, size=num_samples)\n",
    "    candidates = torch.tensor(np.stack([x_candidates, y_candidates], axis=1), dtype=torch.float32)\n",
    "    \n",
    "    squared_norm = torch.sum(candidates**2, dim=-1)\n",
    "    intensity = kappa * torch.exp(-squared_norm / scale**2)\n",
    "    \n",
    "    uniform_samples = torch.rand(num_samples)  # Uniform samples for rejection\n",
    "    acceptance_mask = uniform_samples < (intensity / kappa)\n",
    "    \n",
    "    accepted_points = candidates[acceptance_mask]\n",
    "    return accepted_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_kappa = 100.0\n",
    "true_scale = 0.5\n",
    "region = ((0, 1), (0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\79217\\AppData\\Local\\Temp\\ipykernel_21680\\2803613514.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  samples.append(torch.tensor(x_t))\n"
     ]
    }
   ],
   "source": [
    "num_samples = 1000\n",
    "samples = []\n",
    "for _ in range(num_samples):\n",
    "    x_t = generate_poisson_points(true_kappa, true_scale, region)\n",
    "    samples.append(torch.tensor(x_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_sequence(samples, batch_first=True, padding_value=0)\n",
    "lengths = torch.tensor([len(s) for s in samples], dtype=torch.int64)\n",
    "\n",
    "lengths_expanded = lengths.unsqueeze(-1).expand(-1, X.shape[1])\n",
    "X = torch.cat((X, lengths_expanded.unsqueeze(-1)), dim=-1)\n",
    "\n",
    "X_train = X[:100]\n",
    "X_val = X[100:150]\n",
    "loader_train = FastTensorDataLoader(X_train, batch_size=1000, shuffle=True)\n",
    "loader_val = FastTensorDataLoader(X_val, batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InhomPoissonNLL(nn.Module):\n",
    "    def __init__(self, region):\n",
    "        super().__init__()\n",
    "        self.region = region\n",
    "        self.kappa = nn.Parameter(torch.tensor([np.abs(np.random.randn())], dtype=torch.float32, device=device))\n",
    "        self.scale = nn.Parameter(torch.tensor([np.abs(np.random.randn())], dtype=torch.float32, device=device))\n",
    "\n",
    "    def intensity(self, x, y, epsilon=1e-10):\n",
    "        return self.kappa * torch.exp(-(x**2 + y**2) / self.scale**2) + epsilon\n",
    "\n",
    "    def forward(self, points):\n",
    "        nll = 0.0\n",
    "        (x_min, x_max), (y_min, y_max) = self.region\n",
    "        x_grid = torch.linspace(x_min, x_max, steps=100)\n",
    "        y_grid = torch.linspace(y_min, y_max, steps=100)\n",
    "        \n",
    "        xx, yy = torch.meshgrid(x_grid, y_grid)\n",
    "        partition = torch.trapz(torch.trapz(self.intensity(xx, yy), dx=0.01), dx=0.01)\n",
    "        for point in points:\n",
    "            length = int(point[0, -1])\n",
    "            x, y = point[:length, 0], point[:length, 1]\n",
    "            log_likelihood = torch.sum(torch.log(self.intensity(x, y)))\n",
    "            \n",
    "            # Add to the total NLL\n",
    "            nll += log_likelihood - partition\n",
    "        \n",
    "        return -nll "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoissonSM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.tensor([torch.abs(torch.randn(1))], dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x):\n",
    "        squared_norm = torch.sum(x**2, dim=-1)\n",
    "        return - squared_norm / self.scale**2\n",
    "\n",
    "    def compute_psi(self, x):\n",
    "        x.requires_grad_()\n",
    "        nn_output = self.forward(x)\n",
    "        psi = torch.autograd.grad(nn_output, x, grad_outputs=torch.ones_like(nn_output), create_graph=True)[0]\n",
    "        return psi\n",
    "\n",
    "    def loss(self, points):\n",
    "        lengths = points[:, 0, -1].to(dtype=torch.int64)\n",
    "        max_length = lengths.max()\n",
    "        x_t = points[:, :max_length, :-1]  # Pad to max length in batch\n",
    " \n",
    "        psi_x = self.compute_psi(x_t)\n",
    "        norm_squared = (psi_x ** 2).sum(dim=-1)  # Sum across all dimensions\n",
    "\n",
    "        # padded values give none zero divergence -> mask \n",
    "        mask = torch.arange(max_length, device=x_t.device).unsqueeze(0) < lengths.unsqueeze(1)\n",
    "        divergence = 0\n",
    "        for i in range(x_t.shape[-1]):  # Iterate over the features of x\n",
    "            gradient = torch.autograd.grad(psi_x[..., i].sum(), x_t, retain_graph=True, create_graph=True)[0]\n",
    "            divergence += gradient[..., i]  # Sum over each feature dimension\n",
    "        \n",
    "        divergence = divergence * mask\n",
    "        total_loss = 0.5 * norm_squared + divergence\n",
    "        total_loss = total_loss.sum(dim=-1)/lengths  # Sum over the time dimension\n",
    "        \n",
    "        return total_loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, loader_train, num_epochs=1000):\n",
    "    for _ in tqdm(range(num_epochs), desc=\"Running epochs\"):\n",
    "        for X_batch in loader_train:\n",
    "            optimizer.zero_grad()\n",
    "            if isinstance(model, InhomPoissonNLL):\n",
    "                loss = model(X_batch[0])\n",
    "            else:\n",
    "                loss = model.loss(X_batch[0])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nll_model = InhomPoissonNLL(region).to(device)\n",
    "sm_model = PoissonSM().to(device)\n",
    "\n",
    "nll_optimizer = torch.optim.Rprop(nll_model.parameters())\n",
    "sm_optimizer = torch.optim.Rprop(sm_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running epochs:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\79217\\Documents\\TUD\\3 semester\\Research Project\\poisson_inference\\.venv\\Lib\\site-packages\\torch\\functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3596.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Running epochs: 100%|██████████| 1000/1000 [00:59<00:00, 16.78it/s]\n"
     ]
    }
   ],
   "source": [
    "trained_nll = train_model(nll_model, nll_optimizer, loader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running epochs: 100%|██████████| 1000/1000 [00:06<00:00, 163.51it/s]\n"
     ]
    }
   ],
   "source": [
    "trained_sm = train_model(sm_model, sm_optimizer, loader_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of Methods:\n",
      "NLL Method:\n",
      "  Estimated kappa: 102.14119720458984\n",
      "  Estimated scale: -0.5042386651039124\n",
      "\n",
      "SM Method:\n",
      "  Estimated scale: 0.4922265410423279\n"
     ]
    }
   ],
   "source": [
    "nll_results = {\"kappa\": trained_nll.kappa.item(), \"scale\": trained_nll.scale.item()}\n",
    "sm_results = {\"scale\": trained_sm.scale.item()}\n",
    "\n",
    "\n",
    "# Print Results\n",
    "print(\"Comparison of Methods:\")\n",
    "print(\"NLL Method:\")\n",
    "print(f\"  Estimated kappa: {nll_results['kappa']}\")\n",
    "print(f\"  Estimated scale: {nll_results['scale']}\")\n",
    "\n",
    "print(\"\\nSM Method:\")\n",
    "print(f\"  Estimated scale: {sm_results['scale']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
